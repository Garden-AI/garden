
## Tutorial: Develop a Garden from Scratch

This tutorial will guide you through developing a Garden from scratch: registering a Model, creating and registering a Pipeline, adding the Pipeline to your Garden, and publishing your Garden. It will then show you how to use published Gardens, including how to find a published Garden and execute its pipeline(s) on a remote Globus Compute endpoint of choice.

#### Prerequisites

1. A pre-trained machine learning model saved in a format compatible with the SDK, i.e., `sklearn`, `pytorch`, or `tensorflow`.
2. The Garden CLI [installed](user_guide/installation) on your system.


### Create a Pipeline

We start by developing a Pipeline, which will contain of all the code necessary to e.g. clean some input data and make a prediction using the pre-trained model we want to make reproducible and accessible by others.

A pipeline is typically defined in its own python module or file, and the `garden-ai pipeline create` command will generate a partially-filled `pipeline_name/pipeline.py` file, with templated starter code you can easily complete. For example:

```bash
garden-ai pipeline create looking_glass_pipeline \
        --author "Dee, Tweedle" \
        --contributor "Dum, Tweedle" \
        --description "..."
```

This will make a template python file `looking_glass_pipeline/pipeline.py`, and a blank `looking_glass_pipeline/requirements.txt` where you should pin any of the dependencies you'll need in the Pipeline's container. The `pipeline.py` file generated by the CLI will contain some blank `@step`-decorated functions and will instantiate a `Pipeline` at the end.

> [!NOTE] Note
> You don't need to pin `garden-ai` as a requirement. Also, make sure to pin exact versions of the AI/ML frameworks used to train any models used in the pipeline, to ensure reproducibility in the future.


Here's what a very simple `pipeline.py` might look like when it's (nearly) complete:

```python

client = GardenClient()

@step
def preprocessing(input_data: pd.DataFrame) -> pd.DataFrame:
    """do some preprocessing"""
    filtered_data = input_data["important column"]
    return filtered_data

@step
def run_inference(
    cleaned_data: pd.DataFrame,
    model=Model("YOUR MODEL NAME HERE"),  # See "Register Model" section below
) -> np.ndarray:
    """running some inference"""
    results = model.predict(cleaned_data)
    return results

# instantiate the pipeline
looking_glass_pipeline: Pipeline = client.create_pipeline(
    title="Looking Glass Pipeline",
    doi="10.26311/fake-doi",
    steps=(preprocessing, run_inference),  # composes the fns defined above
    requirements_file="/full/path/to/requirements.txt",
    authors=["Dee, Tweedle", "et al."],
    contributors=["Dum, Tweedle"],
    description="Makes you feel a little sorry for the poor oysters",
    tags=["Carpentry", "Walrus Studies"],
)
```


> [!NOTE] Note
> The only code that will be serialized and registered as part of the pipeline is code that appears _in the body_ of one of its steps; this means that you should put your imports like `import pandas as pd` inside the functions that use them, not at the top level of the module where they'd otherwise go.

> [!NOTE] Note
> Eventually, the pipeline will be accessed from a Garden using the same variable name found in this python file, e.g. `my_garden.looking_glass_pipeline(...)`. To customize this, you can specify a different `short_name` as a kwarg in the constructor (setting the pipeline's name in any Garden) or by specifying an `--alias` when you add the pipeline to a Garden (setting its name for only that Garden).

When we're ready to test this pipeline locally before registering it with the Garden service, we could import the pipeline and call it directly:

```
>>> from pipeline import looking_glass_pipeline
>>> test_result = looking_glass_pipeline(my_test_data)
```


### Register a Model

In our pipeline's templated code, notice that one step had the default argument `model=Model("YOUR MODEL NAME HERE")`. This `Model` argument can be used in any step, and accepts the "full name" or URI of a pre-trained model which has been registered with the Garden service.

Use the `garden-ai model register` subcommand to register your model:

```bash
garden-ai model register "my-model-name" /path/to/model.pkl sklearn
```
> [!NOTE] Note
> If you have a model saved in a specific serialization format such as joblib or a pickle file, you can specify it with the optional CLI argument `--serialize-type`. Here is an example using joblib `garden-ai model register "my-model-name" /path/to/model.pkl sklearn --serialize-type joblib`. If not provided an attempt at using a flavor-compatible default will be made.

This command will register your model with the Garden service and return a "full name", which is the name you chose prefixed with your email address, e.g. `"me@institution.edu/my-model-name"`. This full model name is how you can reference the registered model in your pipeline code:

```python
...

@step
def run_inference(
    cleaned_data: pd.DataFrame,
    model=Model("me@institution.edu/my-model-name"),
) -> np.ndarray:
    """running some inference"""
    results = model.predict(cleaned_data)
    return results

```


> [!NOTE]
> Regardless of "flavor" (sklearn, pytorch, etc) used when registering the underlying ML model, the `Model` object will appear to have a single `predict` method which passes its input directly to your model and returns its prediction.


### Register a Pipeline

Once we're satisfied with our pipeline's definition in `pipeline.py`, we can register the pipeline, allowing us to:

 - Automatically define a container for the pipeline to execute in, which will contain all the dependencies pinned in the pipeline's `requirements.txt`.
 - Mint a DOI for the Pipeline using [DataCite](https://datacite.org).
 - Add the Pipeline to a `Garden`, making it easy to publish/cite and for others to reproduce your work.
 - Execute the pipeline on a remote Globus Compute endpoint.

We do this with the CLI:
```bash
garden-ai pipeline register looking_glass_pipeline/pipeline.py
```

> [!NOTE] Note
> Only registered Pipelines can be added to a Garden and executed remotely; only local (i.e. importable) Pipelines can run without an endpoint.

### Create Your Garden

With your pipeline registered, it's time to create a Garden to house your pipeline(s). Use the Garden CLI to create your Garden:

```bash
garden-ai garden create \
	--title "Garden of Live Flowers" \
	--author "The Red Queen" --year 1871
```

The output of this will give you a DOI you can use to reference this garden in other commands.

### Add Pipeline to Your Garden

You can add your registered pipeline to your newly created garden using the `garden add-pipeline` subcommand, like so:

```bash
garden-ai garden add-pipeline \
	--garden='10.garden/doi' \
	--pipeline='10.pipeline/doi'
```


> [!NOTE] Note
> If adding the pipeline to an already-published Garden, you specify the Garden by its DOI. You'll need to publish the Garden again for others to see the new Pipeline, however.

### Publish Your Garden

Finally, after creating your Garden and adding the pipeline, it's time to publish your Garden:

```bash
garden-ai garden publish --garden='10.garden/doi'
```

The output of this command will contain a DOI, which you can use to cite and share your Garden, as shown in the next section of this tutorial.

Congratulations! You've just developed a Garden from scratch. Now your work is findable, accessible, interoperable and reusable ğŸŒ±.

## Using Published Gardens

Now that your Garden is published, let's see how you (or others) can find and use published Gardens.

### Discover a Garden

You can find a published Garden by searching for it using the CLI. This would list all published Gardens with `"Dee, Tweedle"` listed as an author (substitute your own name to find your garden from part 1):

```bash
garden-ai garden search --author "Dee, Tweedle"
```

Grabbing just the DOI from the output of that command (or from anywhere else this Garden may have been cited), we have everything we need to execute the pipeline on a choice Globus Compute endpoint:

```python
>>> gc = GardenClient()
>>> found_garden = gc.get_garden_by_doi('10.garden/doi')
```

### Remotely Execute a Pipeline

Once you have a Garden, you can execute any of its pipelines remotely. Make sure to specify a valid Globus Compute endpoint (or use the default tutorial endpoint):

```python
>>> my_data = pd.DataFrame(...)
>>> tutorial_endpoint = "86a47061-f3d9-44f0-90dc-56ddc642c000"
>>> results = found_garden.looking_glass_pipeline(my_data, endpoint=tutorial_endpoint)
# ... executing remotely on endpoint 86a47061-f3d9-44f0-90dc-56ddc642c000
>>> print(results)  # neat!
```

That's all there is to it! You've just developed and published your own Garden, and learned how to use published Gardens to remotely execute and reproduce your work. Happy Gardening! ğŸŒ±
